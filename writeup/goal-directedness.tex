\documentclass{article}

\usepackage[margin=0.9in]{geometry}
\usepackage{parskip}

\usepackage{mathtools,amssymb}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}


\usepackage{parskip}

\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\Ex}{\mathbb E}
\newcommand\commentout[1]{}

\begin{document}
    \commentout{
        \begin{table}
            \centering
            \renewcommand\arraystretch{1.2}
            \begin{tabular}{r|cc}
                & \multicolumn{2}{c}{A} \\\hline
                \multirow{ 2}{*}{\rotatebox{90}{A}}&
                    %1.
                    % Specify behavior directly (the trace of a program)
                    Behavior ($\tau : S^* \to A^*$)
                    &
                    %2.
                    % Specify behavior in a higher level programming language
                    Policy ($\pi: S \to A$)
                    \\&
                    %3.
                    % Specify a metric by which you judge behavior, and train systems to maximize this metric.
                    Desirability ($S^* \times A^* \to U$)
                    &
                    % 4.
                    Reward function ($A \times S \to U$)
            \end{tabular}
        \end{table}}

    % One way of building a program is to specify its behavior directly.
    % A more compact way of doing this is to generate code, which unfolds into behavior.
    
    % \section{Review of Markov Decision Processes}
    \section{Introduction}
    
    \textbf{A Review of Markov Decision Processes.}
    Let's consider an agent that, at each timestep $t=1, 2,\ldots$, observes the state $s^{(t)} \in S$ of the system, and takes an action $a^{(t)} \in A$. 
    Suppose further that the system dynamics are controlled by a fixed \emph{transition} map $\tau: S \times A \to \Delta S$ such that, for all $t$, the next state $s^{(t+1)}$ is drawn independently from the distribution $\tau(s^{(t)}, a^{(t)})$. Bending the standard terminology slightly, we will refer to
    % $(S, A, \tau: S \times A \to \Delta S)$ 
    $(S, A, \tau)$---or simply $\tau$, which implicitly also determines $A$ and $S$---as an \emph{environment}. 
    
    % How should one go about specifying the behavior of an agent in this system?  
    The agent's \emph{behavior} in an environment $(S, A, \tau)$ is the sequence of contextual actions 
    % $(s^{(t)}, a^{(t)})_{t = 1, 2, \ldots}$. 
    $\{ s^{(t)} \mapsto a^{(t)} \}_{t=1,2,\ldots}$.
    Suppose we want an agent to behave in a certain way. How best should we specify this?
    % How should one go about specifying the behavior?
    One option is to specify each action directly; another is to apply a policy $\pi: S \to \Delta A$ that gives a distribution of actions for each state.    
    % Both approaches require a great deal of engineering. 
    The latter approach is like specifying a program instead of all possible execution traces, and so is more tractable. 
    Supposing that the state space $S$ encodes everything relevant to action, such as any internal state of the agent, specifying a policy instead of behavior should be an uncontroversial simplification. 
    Still, specifying a policy directly requires a great deal of engineering:
    
    \begin{enumerate}
        \item one has to separately decide on a distribution of actions for every state, and there may not be a good ``default'' action.
        \item  As environment complexity grows, it quickly becomes arduous to specify behavior that requires thinking multiple time steps in the future. 
        \item Such a behavioral specification is also arguably quite brittle, since the entire behavior can be permanently derailed by mis-specification policy at a single state.
    \end{enumerate}
        
    The standard approach is to instead specify what you \emph{want}, in the form of a \emph{reward function} $R : S \times A \times S \to \mathbb R$. Let $(\tau\pi)^*$ denote the unique distribution over trajectories $(\mathbf s, \mathbf a) = \{(s^{(t)}, a^{(t)}\}_{t =1,2,\ldots}$ generated by following policy $\pi$ in environment $\tau$. The goal is to automatically learn the policy which maximizes total reward, 
    % $\sum_{t} R(s^{(t)}, a^{(t)}, s^{(t+1)})$. 
    \begin{equation}\label{eq:total-expected-reward}
        \Ex_{\mathbf {s,a} \sim (\pi\tau)^*} \sum_{t=1,2\ldots} R(s^{(t)}, a^{(t)}, s^{(t+1)})
    %    
    \end{equation}
    ---although, because the sum in \eqref{eq:total-expected-reward} when it diverges, it is typically convenient to choose a \emph{discount} rate $\gamma \in (0,1)$, and instead search for a policy maximizing the expected \emph{discounted} reward:
    % $
    %     \sum_{t} \gamma^t R(s^{(t)}, a^{(t)}, s^{(t+1)})
    % $.
    \begin{equation}\label{eq:expected-discounted-reward}
        \Ex_{\mathbf {s,a} \sim (\pi\tau)^*} \sum_{t=1,2\ldots} \gamma^t R(s^{(t)}, a^{(t)}, s^{(t+1)}) 
        .
    \end{equation}
    This motivates the standard definition of a Markov Decision Process (MDP) $(S, A, \tau, R, \gamma)$, and the notion of ``solving'' an MDP, which is to say, finding an optimal policy $\pi^* : S \to \Delta A$ which maximizes the total discounted reward.    
    
    
    \textbf{Concerns and Foreshadowing.}    
    Given that $R$ is a \emph{larger} object than $\pi$,%
        \footnote{$R \in \mathbb R^{S \times A \times S}$, while $\pi \in (\Delta A)^S \cong \mathbb R_{\ge 0}^{S \times (|A|-1)}$, so if there are $N$ states and $M$ actions, $R$ is a vector of $N^2 M$ real numbers, while $\pi$ is a vector of $N(M-1)$ numbers}
    it may seem counter-intuitive that this approach would address the problems above, but it has a critical advantage: a reasonable default of zero.
    This means a designer who wants to specify complex behavior needed to achieve a \textbf{goal}, needs only to think about the goal and the states/actions associated with them, and not the entire process of how one gets there. 
    
    As much as this buys us, it does not save us from the fact that, once again, subtle mis-specifications of a reward can result in dramatic changes in behavior. 
    Moreover, while simple reward functions may be a much more compact and robust description of behavior than a full policy, 
    
    The notion of a \textbf{goal} has intentionally been left ambiguous, since the present paper is 
    
    
    
        
    % \[ 
    %     \argmax_{\pi : S \to \Delta A}
    %         \sum_{t=1}^{\infty}\Ex_{a \sim \pi} R()
    % \]
    
        
    
    \subsection{More }
    Given a fixed policy $\pi$ and reward function $R$, we can define a \emph{Value} function $V_{\pi, R} : S \to \mathbb R$ its expected future reward.
    It satisfies the recursive relation
    \begin{align*}
        V_{\pi,R}(s) = \Ex_{ \substack{ a \sim \pi(A|s) \\ s' \sim \tau(S|a,s) }}
            \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    \end{align*}
    Embedded inside is a standard notion of an action-dependent value, or $Q$-value,
    \[
        Q_{\pi,R}(s,a) = \Ex_{ s' \sim \tau(S|a,s) } \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    \]
    so that $V_{\pi,R}(s) = \Ex_{a \sim \pi(A|s)} Q_{\pi,R}(a,s)$. 
    % that does not have the dependence on .
    % It satisfies the recursive relation
    % \begin{align*}
    %     V_{\pi,R}(s) = \Ex_{ \substack{ a \sim \pi(A|s) \\ s' \sim \tau(S|a,s) }}
    %         \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    %     Q(a,s) = \Ex_{} \Ex_{s' \sim \tau(S|a,s)} \Big[ R(s,a,s') + \gamma V^*_R(s') \Big]
    % \end{align*}

    There's also a notion of value
    \[
        V^*_R(s)  = \max_{a \in A}
    \]


    Advantage $\mathcal A$
    
    
    \section{Notions of Goal-Directedness}

    
    
    For a given MDP, one might wonder if the optimal policy 

    The value of $\gamma$ clearly has something to do 
    
    
    
    
    
    

    \section{Experiments}
\end{document}
