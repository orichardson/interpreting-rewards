\documentclass{article}

\usepackage[margin=0.9in]{geometry}
\usepackage{mathtools,amssymb}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}

\DeclareMathOperator{\argmax}{\arg\max}
\newcommand\commentout[1]{}

\begin{document}  
    \commentout{
        \begin{table}
            \centering
            \renewcommand\arraystretch{1.2}
            \begin{tabular}{r|cc}
                & \multicolumn{2}{c}{A} \\\hline
                \multirow{ 2}{*}{\rotatebox{90}{A}}&
                    %1.
                    % Specify behavior directly (the trace of a program) 
                    Behavior ($\tau : S^* \to A^*$)
                    & 
                    %2.
                    % Specify behavior in a higher level programming language
                    Policy ($\pi: S \to A$)
                    \\&
                    %3.
                    % Specify a metric by which you judge behavior, and train systems to maximize this metric.
                    Desirability ($S^* \times A^* \to U$)
                    &
                    % 4. 
                    Reward function ($A \times S \to U$)
            \end{tabular}
        \end{table}}
    
    % One way of building a program is to specify its behavior directly. 
    % A more compact way of doing this is to generate code, which unfolds into behavior.
    
    \section{Review of Markov Decision Processes}
    

    Let's consider an agent that, at each timestep $t$, observes the state $s^{(t)} \in S$ of the system, and takes an action $a^{(t)} \in A$. 
    Suppose further that the system dynamics are controlled by a fixed \emph{transition} map $\tau: S \times A \to \Delta S$ such that, for all $t$, the next state $s^{(t+1)}$ is drawn independently from the distribution $\tau(s^{(t)}, a^{(t)})$.
    
    % How should one go about specifying the behavior of an agent in this system?  
    The agent's behavior is the sequence of contextual actions 
    % $(s^{(t)}, a^{(t)})_{t = 1, 2, \ldots}$. 
    $\{ s^{(t)} \mapsto a^{(t)} \}_{t=1,2,\ldots}$.
    How should one go about specifying an agent's behavior in this setting?
    One option is to specify each action directly; another is to apply a policy $\pi: S \to \Delta A$ that gives a distribution of actions for each state.
    Both approaches require a great deal of engineering. 
    
    The more common approach is to instead specify a \emph{reward function} $R : S \times A \times S \to \mathbb R$, and try to automatically learn the policy which maximizes total reward, $\sum_{t} R(s^{(t)}, a^{(t)}, s^{(t+1)})$. 
    Because it is hard to reason about this sum when it diverges, it is typically more convenient to choose a \emph{discount} rate $\gamma \in (0,1)$ , and find a policy that maximizes maximize \emph{discounted} reward:
    $
        \sum_{t} \gamma^t R(s^{(t)}, a^{(t)}, s^{(t+1)})
    $.
        
    This motivates the standard definition of a Markov Decision Process (MDP) $(S, A, \gamma, \tau, R)$, and the notion of ``solving'' an MDP, which is to say, finding an optimal policy
    
    \[ 
        \argmax_{\pi : S \to \Delta A}
    \]
        
    
    
    \section{Notions of Goal-Directedness}
    
    Suppose you have a 
    
    
    
    
    
\end{document}
