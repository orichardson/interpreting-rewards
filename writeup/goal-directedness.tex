\documentclass{article}

\usepackage[margin=0.9in]{geometry}
\usepackage{parskip}

\usepackage{mathtools,amssymb}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage[colorlinks=true]{hyperref}
\usepackage{cleveref}
\crefname{reward}{Reward}{Rewards}


\usepackage{parskip}

\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\Ex}{\mathbb E}
\newcommand{\thickD}{I\mkern-8muD}
\newcommand\commentout[1]{}

\begin{document}
    \commentout{
        \begin{table}
            \centering
            \renewcommand\arraystretch{1.2}
            \begin{tabular}{r|cc}
                & \multicolumn{2}{c}{A} \\\hline
                \multirow{ 2}{*}{\rotatebox{90}{A}}&
                    %1.
                    % Specify behavior directly (the trace of a program)
                    Behavior ($\tau : S^* \to A^*$)
                    &
                    %2.
                    % Specify behavior in a higher level programming language
                    Policy ($\pi: S \to A$)
                    \\&
                    %3.
                    % Specify a metric by which you judge behavior, and train systems to maximize this metric.
                    Desirability ($S^* \times A^* \to U$)
                    &
                    % 4.
                    Reward function ($A \times S \to U$)
            \end{tabular}
        \end{table}}

    % One way of building a program is to specify its behavior directly.
    % A more compact way of doing this is to generate code, which unfolds into behavior.

    % \section{Review of Markov Decision Processes}
    \section{Introduction}

    \textbf{A Review of Markov Decision Processes.}
    Let's consider an agent that, at each timestep $t=1, 2,\ldots$, observes the state $s^{(t)} \in S$ of the system, and takes an action $a^{(t)} \in A$.
    Suppose further that the system dynamics are controlled by a fixed \emph{transition} map $\tau: S \times A \to \Delta S$ such that, for all $t$, the next state $s^{(t+1)}$ is drawn independently from the distribution $\tau(s^{(t)}, a^{(t)})$. Bending the standard terminology slightly, we will refer to
    % $(S, A, \tau: S \times A \to \Delta S)$
    $(S, A, \tau)$---or simply $\tau$, which implicitly also determines $A$ and $S$---as an \emph{environment}.

    % How should one go about specifying the behavior of an agent in this system?
    The agent's \emph{behavior} in an environment $(S, A, \tau)$ is the sequence of contextual actions
    % $(s^{(t)}, a^{(t)})_{t = 1, 2, \ldots}$.
    $\{ s^{(t)} \mapsto a^{(t)} \}_{t=1,2,\ldots}$.
    Suppose we want an agent to behave in a certain way. How best should we specify this?
    % How should one go about specifying the behavior?
    One option is to specify each action directly; another is to apply a policy $\pi: S \to \Delta A$ that gives a distribution of actions for each state.
    % Both approaches require a great deal of engineering.
    The latter approach is like specifying a program instead of all possible execution traces, and so is more tractable.
    Supposing that the state space $S$ encodes everything relevant to action, such as any internal state of the agent, specifying a policy instead of behavior should be an uncontroversial simplification.
    Still, specifying a policy directly is quit difficult:

    \begin{enumerate}
        \item one has to separately decide on a distribution of actions for every state, and there may not be a good ``default'' action.
        \item  As environment complexity grows, it quickly becomes arduous to specify behavior that requires thinking multiple time steps in the future.
        \item Such a behavioral specification is also arguably quite brittle, since the entire behavior can be permanently derailed by mis-specification policy at a single state.
    \end{enumerate}

    The standard approach is to instead specify what you \emph{want}, in the form of a \emph{reward function} $R : S \times A \times S \to \mathbb R$. Let $(\tau\pi)^*$ denote the unique distribution over trajectories $(\mathbf s, \mathbf a) = \{(s^{(t)}, a^{(t)}\}_{t =1,2,\ldots}$ generated by following policy $\pi$ in environment $\tau$. The goal is to automatically learn the policy which maximizes total reward,
    % $\sum_{t} R(s^{(t)}, a^{(t)}, s^{(t+1)})$.
    \begin{equation}\label{eq:total-expected-reward}
        \Ex_{\mathbf {s,a} \sim (\pi\tau)^*} \sum_{t=1,2\ldots} R(s^{(t)}, a^{(t)}, s^{(t+1)})
    %
    \end{equation}
    ---although, because the sum in \eqref{eq:total-expected-reward} when it diverges, it is typically convenient to instead fix a \emph{discount} rate $\gamma \in (0,1)$, and search for a policy maximizing the expected \emph{discounted} reward:
    % $
    %     \sum_{t} \gamma^t R(s^{(t)}, a^{(t)}, s^{(t+1)})
    % $.
    \begin{equation}\label{eq:expected-discounted-reward}
        \Ex_{\mathbf {s,a} \sim (\pi\tau)^*} \sum_{t=1,2\ldots} \gamma^t R(s^{(t)}, a^{(t)}, s^{(t+1)})
        .
    \end{equation}
    This gives us the standard notion of a Markov Decision Process (MDP) $(S, A, \tau, R, \gamma)$, and the corresponding problem of finding an optimal policy $\pi^* : S \to \Delta A$ which maximizes the total discounted reward.


    \subsection{Problems with Reward Functions}
        \label{sec:concern-reward}
    Although still better than
    Given that $R$ is a \emph{larger} object than $\pi$,%
        \footnote{$R \in \mathbb R^{S \times A \times S}$, while $\pi \in (\Delta A)^S \cong \mathbb R_{\ge 0}^{S \times (|A|-1)}$, so if there are $N$ states and $M$ actions, $R$ is a vector of $N^2 M$ real numbers, while $\pi$ is a vector of $N(M-1)$ numbers}
    it may seem counter-intuitive that this approach would address the problems above, but it has a critical advantage: a reasonable default of zero.
    This means a designer who wants to specify complex behavior needed to achieve a \textbf{goal}, needs only to think about the goal and the states/actions associated with them, and not the entire process of how one gets there.

    As much as this buys us, it does not save us from the fact that, once again, subtle mis-specifications of a reward can result in dramatic changes in behavior.
    Moreover, while simple reward functions may be a much more compact and robust description of behavior than a full policy,

    The notion of a \textbf{goal} has intentionally been left ambiguous, since the present paper is




    % \[
    %     \argmax_{\pi : S \to \Delta A}
    %         \sum_{t=1}^{\infty}\Ex_{a \sim \pi} R()
    % \]



    \subsection{More Preliminaries and Standard Concepts for MDPs}
    Fix an environment $(S,A,\tau)$.
    Given a specific discount factor $\gamma$, reward function $R$, and policy $\pi$, not all states will be equally valuable. For an ext.
    We can define a \emph{value function} $V_{\pi, R} : S \to \mathbb R$.
    It satisfies the recursive relation
    \begin{align*}
        V_{\pi,R}(s) = \Ex_{ \substack{ a \sim \pi(A|s) \\ s' \sim \tau(S|a,s) }}
            \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    \end{align*}
    Embedded inside it is a standard notion of an action-dependent value, or $Q$-value,
    \[
        Q_{\pi,R}(s,a) = \Ex_{ s' \sim \tau(S|a,s) } \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    \]
    so that $V_{\pi,R}(s) = \Ex_{a \sim \pi(A|s)} Q_{\pi,R}(a,s)$.
    The optimal policy is then
    \[
        \pi^*(a|s) = \argmax_{\pi} Q_{\pi,R}(a,s) - V(s)
    \]

    % that does not have the dependence on .
    % It satisfies the recursive relation
    % \begin{align*}
    %     V_{\pi,R}(s) = \Ex_{ \substack{ a \sim \pi(A|s) \\ s' \sim \tau(S|a,s) }}
    %         \Big[ R(s,a,s') + \gamma V_{\pi, R}(s') \Big]
    %     Q(a,s) = \Ex_{} \Ex_{s' \sim \tau(S|a,s)} \Big[ R(s,a,s') + \gamma V^*_R(s') \Big]
    % \end{align*}

    In a slightly different formulation
    \[
        V^*_R(s)  = \max_{a \in A} \Ex_{ s' \sim \tau(S|a,s) } \Big[ R(s,a,s') + \gamma V^*_{R}(s') \Big]
    \]
    So long as states and rewards are finite, this equation will have a least fixed point, which can be found by iteratively applying the equation.

    The advantage $\mathcal A$

    \section{Notions of Goal-Directedness}

    In \cref{sec:concern-reward}, we mentioned that there are actually more degrees of freedom in a reward function than in a policy.
    So, while it is true that reward functions make it easy to simply articulate goal-directed behavior, they are more flexible.

    Here are some extreme examples:
    \begin{enumerate}
        \item \textbf{Novelty:} Use $R(s, a, s') := - \log \tau(s'|a,s)$, the surprisal of the new state, given the old state and action. Or, to simulate learning when the environment is unknown, train a model $\hat \tau (s'|a,s)$ of the transition dynamics, and use $\hat\tau$ in place of $\tau$.
            \label[reward]{item:novelty}
        \item \textbf{Goal States}: $R(s,a,s') = 1$ iff $s'$ is the goal state $s^*$, and zero otherwise.
            \label[reward]{item:goalstate}
        \item \textbf{Encoding a Policy:} Given a deterministic policy $\pi_{\text{det}} : S \to A$, set $R(s, a, s') = 1$ if $a = \pi_{\text{det}}(s)$ and zero otherwise.
            \label[reward]{item:detpolicy}
    \end{enumerate}

    % On one extreme, a reward function can clearly encode a deterministic policy $\pi_{\text{det}} : S \to A$.
    % For a given MDP, one might wonder
    % if the optimal policy

    The last two examples are in some ways opposites of one another. To get to a goal state, one may have to think far ahead, and chain together a large number of actions. It may require planning.
    On the other hand, a policy can be followed \emph{myopically}.

    Since we have argued that policies are harder to understand than goals (which is why we decided to specify reward functions instead of policies in the first place), we are interested in characterizing reward functions that are interpretable in this way.
    It seems that one aspect of interpretablility is the degree of \emph{goal-directedness}---perhaps to be understood as the opposite of myopia---that a given reward function induces.

    First, two observations about the formalism we already have:
    \begin{enumerate}
        \item The value of $\gamma$ clearly has something to do with myopia: as $\gamma \to 0$, one takes actions based only on the immediate next reward, and making it myopic. The optimal policy for \cref{item:detpolicy}, for instance, does not depend on $\gamma$, while the optimal policy for \cref{item:goalstate} is very sensitive to $\gamma$, at least if there are multiple goal states or a small amount of noise.
        \item The value function $V(s)$ also captures something about goal-directedness. For \cref{item:detpolicy}, every state has the same value (since you can get the same payoff no matter where you are), but the same definitely cannot be said for \cref{item:goalstate}.
    \end{enumerate}

    Accordingly, we analyze the following measures of goal-directedness, both requiring a distribution $D(S)$ over states.
    \begin{equation}
        \mathit{Diff}_D(R) := \Ex_{s \sim D} \thickD\Big( \pi^*_{R, \epsilon}(A|S) \,\Big\Vert\, \pi^*_{R,1-\epsilon}(A|S) \Big)
    \end{equation}
    for some measure of divergence $\thickD$.

    \begin{equation}
        \mathit{ValVar}_D(R) := \mathbb V_{s \sim D} \Big[ V^*_R(s) \Big]
    \end{equation}


    \section{Experimental Setup}

    Gridworld

    \section{Analysis}
    \subsection{Value Variance}
    \subsection{Gamma Variation}
\end{document}
